{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ddeb67",
   "metadata": {},
   "source": [
    "# Hyperparameters Optimization for the 1st Model\n",
    "## *Non-Autoregressive LSTM*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87b572e",
   "metadata": {},
   "source": [
    "This notebook is used for hyperparameter optimization of the model implemented in LSTMn°4.ipynb.\n",
    "\n",
    "We aim to optimize the LSTM model's hyperparameters by utilizing the full historical dataset (2015–2022) for training and validation.\n",
    "\n",
    "To accelerate the process, we will perform the optimization on a sample of stations, as the objective is to select the parameters that provide the best average performance across the station sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33350f4",
   "metadata": {},
   "source": [
    "## 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea45e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom utility functions used in the project\n",
    "import utils\n",
    "\n",
    "import NAR_models\n",
    "\n",
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c7b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb49d282",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b16d4e",
   "metadata": {},
   "source": [
    "## 2: Hyperparameters Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b62f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "x_train = x_train = pd.read_csv('train_f_x.csv')\n",
    "y_train = pd.read_csv('y_train_sncf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f7842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "df_per_station = utils.prepare_backtest_data(x_train, y_train, remove_covid=True)\n",
    "\n",
    "# Exclude Recent stations\n",
    "RECENT = ['QD6', 'P6E', 'BDC', 'W80', 'W14']\n",
    "df_per_station = utils.filter_stations(df_per_station, RECENT)\n",
    "\n",
    "# Split into train and test dataset\n",
    "df_train = {}\n",
    "df_test = {}\n",
    "for station in df_per_station:\n",
    "    df_train_station, df_test_station = utils.split_dataset(df_per_station[station])\n",
    "    df_train[station] = df_train_station\n",
    "    df_test[station] = df_test_station\n",
    "\n",
    "# Verification\n",
    "print(\"len(df_train):\",len(df_train),\":\",df_train.keys())\n",
    "print(\"len(df_test):\",len(df_test),\":\",df_train.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a776112",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is conducted on a subset of 50 stations (out of 439) to significantly reduce runtime, under the assumption that optimal hyperparameters generalize across stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a248322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample of stations\n",
    "sample_size = 50\n",
    "seed = 555\n",
    "\n",
    "# train\n",
    "sample_train = utils.sample_stations(df_train, sample_size, seed)\n",
    "print(\"sample_train: \", sample_train.keys())\n",
    "\n",
    "# test\n",
    "sample_test = {\n",
    "    station: df_test[station].copy()\n",
    "    for station in sample_train.keys()\n",
    "}\n",
    "\n",
    "print(\"sample_test: \", sample_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a941a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep true values \n",
    "sample_test_true = {\n",
    "    station: df_test[station].copy()\n",
    "    for station in sample_test.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f62824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function to minimize the average MAPE score \n",
    "    across the station sample.\n",
    "    \"\"\"\n",
    "    # 1. Define the search space\n",
    "    params = {\n",
    "        \"units\": trial.suggest_int(\"units\", 40, 80, step=4),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True),\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 16, 56, step=10),\n",
    "        \"seq_len\": trial.suggest_int(\"seq_len\", 30, 120, step=10)\n",
    "    }\n",
    "\n",
    "    # 2. Create copies of the test data to prevent trials from overwriting \n",
    "    # the original sample_test dictionary\n",
    "    trial_sample_test = copy.deepcopy(sample_test)\n",
    "\n",
    "    try:\n",
    "        # 3. Call backtest_lstm with suggested parameters\n",
    "        # We ignore the returned df and losses to save memory during optimization\n",
    "        _, mape_results, _ = NAR_models.backtest_lstm(\n",
    "            sample_train, \n",
    "            trial_sample_test, \n",
    "            sample_test_true, \n",
    "            sample_size,\n",
    "            seq_len=params[\"seq_len\"],\n",
    "            units=params[\"units\"],\n",
    "            activation='tanh',\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            epochs=100, \n",
    "            keep_percentage=0.5,\n",
    "            early_stop=True, \n",
    "            features=['job', 'ferie', 'vacances']\n",
    "        )\n",
    "\n",
    "        # 4. Handle failed trials within the backtest\n",
    "        if not mape_results:\n",
    "            return float('inf')\n",
    "\n",
    "        # 5. Calculate the MEAN MAPE across all stations in the sample\n",
    "        # This makes the hyperparameters generalize better across different stations\n",
    "        all_mapes = [res['MAPE'] for res in mape_results]\n",
    "        average_mape = np.mean(all_mapes)\n",
    "        \n",
    "        return average_mape\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed with error: {e}\")\n",
    "        return float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d6da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution\n",
    "run = 0\n",
    "if (run == 1):\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler())\n",
    "    study.optimize(lambda trial: objective(trial), n_trials=30)\n",
    "else:\n",
    "    print(\"run == 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8944e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best average MAPE:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757632ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters: {'units': 64, 'learning_rate': 0.00612398082698137, 'batch_size': 16, 'seq_len': 120}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
